
---

# ğŸ§  CA6001 Chapter 1 â€“ Supervised vs Unsupervised Learning

*Nanyang Technological University â€“ Dr. Zhang Jiehuang*

---

## ğŸ¯ **Learning Outcomes / å­¦ä¹ ç›®æ ‡**

* äº†è§£å¹¶è§£é‡Šäººå·¥æ™ºèƒ½ (AI) ä¸æ•°æ®ç§‘å­¦ (Data Science) çš„åŸºæœ¬åŸç†
* æŒæ¡ç›‘ç£å­¦ä¹  (Supervised Learning) ä¸éç›‘ç£å­¦ä¹  (Unsupervised Learning) çš„åŒºåˆ«ä¸åº”ç”¨åœºæ™¯
* å­¦ä¼šçº¿æ€§å›å½’ (Linear Regression) ä¸é€»è¾‘å›å½’ (Logistic Regression) çš„æ•°å­¦æ¨¡å‹
* ç†è§£èšç±» (Clustering) çš„åŸç†ä¸å®¢æˆ·åˆ†ç¾¤ (Customer Segmentation) çš„åº”ç”¨
* ç†è§£æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ˆPrecision, Recall, F1-score, AUCï¼‰

---

## ğŸ§© **1. What is AI and Data Science / ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ä¸æ•°æ®ç§‘å­¦**

**English:**
Data Science is an interdisciplinary field that uses data to generate insights and value.
It overlaps with AI, Machine Learning (ML), and Deep Learning (DL).
It applies to domains like healthcare, finance, and marketing.

**ä¸­æ–‡ï¼š**
æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œé€šè¿‡æ•°æ®æ¥åˆ›é€ æ´å¯Ÿä¸ä»·å€¼ã€‚
å®ƒä¸äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç´§å¯†ç›¸å…³ã€‚
åœ¨åŒ»ç–—ã€é‡‘èã€å¸‚åœºç­‰è¡Œä¸šéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚

```python
# Example: Simple data science operation
import pandas as pd

data = {"Age": [25, 30, 45], "Income": [4000, 6000, 10000]}
df = pd.DataFrame(data)
print(df.describe())  # Generate basic insights from data
```

---

## ğŸ§­ **2. Why Supervised & Unsupervised Learning Matter / ä¸ºä»€ä¹ˆç›‘ç£ä¸éç›‘ç£å­¦ä¹ é‡è¦**

* å®ƒä»¬æ„æˆäº† AI/ML çš„åŸºç¡€ï¼Œæ‰€æœ‰é«˜çº§ç®—æ³•éƒ½åŸºäºè¿™ä¸¤ç±»å­¦ä¹ æ–¹å¼ã€‚
* å¯å¹¿æ³›ç”¨äºä¸åŒäº§ä¸šï¼ˆé‡‘èã€åŒ»ç–—ã€å¸‚åœºè¥é”€ç­‰ï¼‰ã€‚
* å¸®åŠ©ä¼ä¸šå’Œç§‘ç ”å†³ç­–è€…åˆ¶å®šåŸºäºæ•°æ®çš„å†³ç­–ã€‚

---

## ğŸ§® **3. Supervised Learning / ç›‘ç£å­¦ä¹ **

### ğŸ§  Definition / å®šä¹‰

**English:**
Supervised Learning uses labeled data `(X, Y)` where `Y` is known.
The goal is to learn the mapping function `f(X) = Y`.

**ä¸­æ–‡ï¼š**
ç›‘ç£å­¦ä¹ ä½¿ç”¨å¸¦æ ‡ç­¾çš„æ•°æ® `(X, Y)`ï¼Œç›®æ ‡æ˜¯å­¦ä¹ è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚

### ğŸ  Example: Linear Regression / çº¿æ€§å›å½’

**Concept:**

* Input (X): house size
* Output (Y): house price
* Goal: Find the best-fit line minimizing error.

**Formula:**
[
f_{w,b}(x) = w x + b
]
Minimize:
[
J(w,b) = \frac{1}{m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2
]

```python
# Linear Regression Example
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

X = np.array([[1000], [1500], [2000], [2500], [3000]])  # sqft
y = np.array([150, 200, 250, 280, 310])  # price in $K

model = LinearRegression()
model.fit(X, y)

plt.scatter(X, y, color="blue", label="Data points")
plt.plot(X, model.predict(X), color="red", label="Best fit line")
plt.xlabel("House Size (sqft)")
plt.ylabel("Price ($K)")
plt.legend()
plt.show()
```

---

### ğŸ“ˆ Logistic Regression / é€»è¾‘å›å½’

**Purpose:**
Used for **classification problems**, mapping outputs between 0 and 1 using the **sigmoid function**:

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

```python
# Logistic Regression Example: Spam Detection
from sklearn.linear_model import LogisticRegression

X = [[50], [100], [200], [250], [300]]   # number of words in email
y = [0, 0, 1, 1, 1]                      # 1 = spam, 0 = not spam

clf = LogisticRegression()
clf.fit(X, y)
print(clf.predict([[150]]))  # Predict if an email with 150 words is spam
```

---

### ğŸ’¸ Application: Financial Fraud Detection / é‡‘èæ¬ºè¯ˆæ£€æµ‹

**Key Idea:**
Use labeled historical transactions (`fraud` / `not fraud`) to train models to detect suspicious activities.
In practice, labels may be rare, so **synthetic labels** and **domain knowledge** are essential.

```python
# Simulated fraud detection example
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

data = pd.DataFrame({
    "amount": [20, 500, 10000, 50, 12000],
    "is_foreign": [0, 0, 1, 0, 1],
    "label": [0, 0, 1, 0, 1]
})

model = RandomForestClassifier()
model.fit(data[["amount", "is_foreign"]], data["label"])
print(model.predict([[7000, 1]]))  # Predict if new transaction is fraud
```

---

## ğŸ” **4. Unsupervised Learning / éç›‘ç£å­¦ä¹ **

### Definition / å®šä¹‰

**English:**
Unsupervised Learning works with unlabeled data, discovering hidden patterns and structures.

**ä¸­æ–‡ï¼š**
éç›‘ç£å­¦ä¹ åœ¨æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®ä¸­å¯»æ‰¾æ½œåœ¨çš„ç»“æ„ä¸è§„å¾‹ã€‚

### Example: Clustering / èšç±»

```python
# K-Means Example: Customer Segmentation
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

X = np.array([[18, 1500], [25, 2500], [40, 4000], [50, 5000], [60, 5500]])
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)

plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)
plt.xlabel("Age")
plt.ylabel("Monthly Spending ($)")
plt.title("Customer Segmentation by KMeans")
plt.show()
```

**Applications / åº”ç”¨é¢†åŸŸï¼š**

* å®¢æˆ·åˆ†ç¾¤ï¼ˆCustomer Segmentationï¼‰
* æ¨èç³»ç»Ÿï¼ˆRecommender Systemsï¼‰
* å¸‚åœºç¯®åˆ†æï¼ˆMarket Basket Analysisï¼‰

---

## âš–ï¸ **5. Comparison: Supervised vs Unsupervised / å¯¹æ¯”æ€»ç»“**

| Feature    | Supervised Learning                       | Unsupervised Learning                 |
| ---------- | ----------------------------------------- | ------------------------------------- |
| Labels     | Uses labeled data                         | Uses unlabeled data                   |
| Goal       | Predict outcomes                          | Find hidden patterns                  |
| Output     | Continuous or categorical                 | Clusters or associations              |
| Algorithms | Linear/Logistic Regression, Decision Tree | K-Means, PCA, Hierarchical Clustering |

---

## âš™ï¸ **6. Evaluation Metrics / æ¨¡å‹è¯„ä¼°æŒ‡æ ‡**

### Train-Test Split & Cross Validation

```python
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LogisticRegression(max_iter=200)
scores = cross_val_score(model, X, y, cv=5)
print("Cross-validation accuracy:", scores.mean())
```

### Precision, Recall, F1-score & Confusion Matrix

```python
from sklearn.metrics import classification_report, confusion_matrix
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

### AUC (Area Under Curve)

```python
from sklearn.metrics import roc_auc_score
import numpy as np

y_true = np.array([0, 0, 1, 1])
y_scores = np.array([0.1, 0.4, 0.35, 0.8])
print("AUC =", roc_auc_score(y_true, y_scores))
```

---

## ğŸ“Š **7. Overfitting vs Underfitting / è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ**

| Type             | Description                                           | Example                                     |
| ---------------- | ----------------------------------------------------- | ------------------------------------------- |
| **Overfitting**  | Model learns noise and performs poorly on unseen data | Too complex model (e.g., too many features) |
| **Underfitting** | Model too simple, fails to capture pattern            | Linear model for nonlinear data             |

```python
# Visual intuition (conceptual, not executable)
# Overfitting: Perfect fit on train data but poor test performance
# Underfitting: Straight line ignoring complex pattern
```

---

## ğŸ§¾ **Summary / æ€»ç»“**

* ç›‘ç£å­¦ä¹ ï¼šè¾“å…¥æœ‰æ ‡ç­¾ï¼Œé€‚ç”¨äºé¢„æµ‹ä¸åˆ†ç±»ã€‚
* éç›‘ç£å­¦ä¹ ï¼šè¾“å…¥æ— æ ‡ç­¾ï¼Œé€‚ç”¨äºèšç±»ä¸æ¨¡å¼å‘ç°ã€‚
* å¸¸è§ç®—æ³•ï¼š

  * ç›‘ç£ï¼šLinear Regression, Logistic Regression, Decision Tree
  * éç›‘ç£ï¼šK-Means, PCA, Association Rules
* æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼šAccuracy, Precision, Recall, F1, AUC
* æ³¨æ„è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆçš„å¹³è¡¡ã€‚

---
